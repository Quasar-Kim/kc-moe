# gin configures for h_transformer_1d encoder-decoder architecture.
#
# Required to be overridden:
#
# - NUM_ENCODER_LAYERS
# - NUM_DECODER_LAYERS
# - NUM_HEADS
# - EMBED_DIM
# - MLP_DIM
# - NUM_EMBEDDINGS
from __gin__ import dynamic_registration

from flax import linen
from flaxformer.architectures.h_transformer import h_attention
from flaxformer.architectures.h_transformer import h_transformer_1d_architecture
from flaxformer.components.attention import dense_attention
from flaxformer.components import dense
from flaxformer.components import embedding
from flaxformer.components import layer_norm

# Must be overridden.
NUM_ENCODER_LAYERS = %gin.REQUIRED
NUM_DECODER_LAYERS = %gin.REQUIRED
NUM_HEADS = %gin.REQUIRED
EMBED_DIM = %gin.REQUIRED
MLP_DIM = %gin.REQUIRED
NUM_EMBEDDINGS = %gin.REQUIRED

# Constants (may be overridden)
ACTIVATION_DTYPE = 'bfloat16'
ACTIVATION_PARTITIONING_DIMS = 1
SCALE = 1.0
DROPOUT_RATE = 0.0
ENCODER_NUM_CLUSTERS = 2
DECODER_NUM_CLUSTERS =16

# Macros
BIAS_INIT = @bias_init/linen.initializers.normal()
bias_init/linen.initializers.normal.stddev = 1e-6
DROPOUT_FACTORY = @dropout_factory/linen.Dropout
dropout_factory/linen.Dropout:
  rate = %DROPOUT_RATE
  broadcast_dims = (-2,)

ARCHITECTURE = @h_transformer_1d_architecture.EncoderDecoder()
h_transformer_1d_architecture.EncoderDecoder:
  encoder_factory = @h_transformer_1d_architecture.Encoder
  decoder_factory = @h_transformer_1d_architecture.Decoder
  shared_token_embedder_factory = @embedding.Embed

# h-transformer Encoder
h_transformer_1d_architecture.Encoder:
  num_layers = %NUM_ENCODER_LAYERS
  layer_factory = @h_transformer_1d_architecture.EncoderLayer
  input_dropout_factory = %DROPOUT_FACTORY
  output_dropout_factory = %DROPOUT_FACTORY
  layer_norm_factory = @layer_norm.T5LayerNorm

# h-transformer Encoder Layer
h_transformer_1d_architecture.EncoderLayer:
  attention = @h_attention.OneDimEncoderSelfAttention()
  mlp = @dense.MlpBlock()
  dropout_factory = %DROPOUT_FACTORY
  layer_norm_factory = @layer_norm.T5LayerNorm

# h-transformer Decoder
h_transformer_1d_architecture.Decoder:
  num_layers = %NUM_DECODER_LAYERS
  layer_factory = @h_transformer_1d_architecture.DecoderLayer
  input_dropout_factory = %DROPOUT_FACTORY
  output_dropout_factory = %DROPOUT_FACTORY
  layer_norm_factory = @layer_norm.T5LayerNorm
  output_logits_factory = @output_logits/dense.DenseGeneral

# Decoupled embedding
output_logits/dense.DenseGeneral:
  features = %NUM_EMBEDDINGS
  use_bias = False
  dtype = 'float32'
  kernel_init = @output_logits_kernel_init/linen.initializers.variance_scaling()
  bias_init = %BIAS_INIT
  kernel_axis_names = ["embed", "vocab"]
output_logits_kernel_init/linen.initializers.variance_scaling:
  scale = %SCALE
  mode = 'fan_in'
  distribution = 'truncated_normal'

# h-transformer Decoder Layer
h_transformer_1d_architecture.DecoderLayer:
  self_attention = @h_attention.OneDimDecoderSelfAttention()
  encoder_decoder_attention = @dense_attention.MultiHeadDotProductAttention()
  mlp = @dense.MlpBlock()
  dropout_factory = %DROPOUT_FACTORY
  layer_norm_factory = @layer_norm.T5LayerNorm

# Token Embedder (shared)
embedding.Embed:
  num_embeddings= %NUM_EMBEDDINGS
  features = %EMBED_DIM
  cast_input_dtype = 'int32'
  dtype = %ACTIVATION_DTYPE
  attend_dtype = 'float32'  # for logit training stability
  embedding_init = @token_embedder_init/linen.initializers.normal()
  one_hot = True
  name = 'token_embedder'
token_embedder_init/linen.initializers.normal.stddev = 1.0

# h-attention EncoderSelfAttention
h_attention.OneDimEncoderSelfAttention:
  num_heads = %NUM_HEADS
  num_clusters = %ENCODER_NUM_CLUSTERS
  dtype = %ACTIVATION_DTYPE
  kernel_init =  @attention_kernel_init/linen.initializers.variance_scaling()
  bias_init = %BIAS_INIT
  use_bias = False
  broadcast_dropout = True
  dropout_rate = %DROPOUT_RATE

# h-attention DecoderSelfAttention
h_attention.OneDimDecoderSelfAttention:
  num_heads = %NUM_HEADS
  num_clusters = %DECODER_NUM_CLUSTERS
  dtype = %ACTIVATION_DTYPE
  kernel_init =  @attention_kernel_init/linen.initializers.variance_scaling()
  bias_init = %BIAS_INIT
  use_bias = False
  broadcast_dropout = True
  dropout_rate = %DROPOUT_RATE

# Dense attention (encoder_decoder_attention)
dense_attention.MultiHeadDotProductAttention:
  num_heads = %NUM_HEADS
  dtype = %ACTIVATION_DTYPE
  kernel_init =  @attention_kernel_init/linen.initializers.variance_scaling()
  bias_init = %BIAS_INIT
  use_bias = False
  broadcast_dropout = True
  dropout_rate = %DROPOUT_RATE
attention_kernel_init/linen.initializers.variance_scaling:
  scale = %SCALE
  mode = 'fan_in'
  distribution = 'normal'

# MLP (encoder, decoder)
dense.MlpBlock:
  use_bias = False
  intermediate_dim = %MLP_DIM
  activations = ('gelu', 'linear')
  kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
  bias_init = %BIAS_INIT
  intermediate_dropout_rate = %DROPOUT_RATE
  final_dropout_rate = 0
  dtype = %ACTIVATION_DTYPE
mlp_kernel_init/linen.initializers.variance_scaling:
  scale = %SCALE
  mode = 'fan_in'
  distribution = 'truncated_normal'

layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
