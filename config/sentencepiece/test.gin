from tokenizer_util import sentence_generator

train:
    model_name = 'test'
    vocab_size = 2000
    n_unused_symbols = 100
    n_extra_symbols = 100
    sentence_iterator = @sentence_generator.generate_retokenized_sentence_from_tfds()

generate_retokenized_sentence_from_tfds:
    dataset = 'tfds_test:1.0.0'
    split = 'train[:10%]'
    text_column = 'text'