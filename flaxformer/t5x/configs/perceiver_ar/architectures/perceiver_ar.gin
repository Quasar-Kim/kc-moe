# Flaxformer Perceiver AR architecture.
#
# Note that xmap is used to work around XLA partitioning issues with gathers.
# If regular vmap is used, a bunch of extra allgathers are added.
#
# Requires the following flags:
# --experimental_xmap_spmd_lowering=True
# --experimental_xmap_spmd_lowering_manual=True
#
# Required to be overridden:
#
# - NUM_LATENTS
# - NUM_DECODER_LAYERS
# - NUM_HEADS
# - HEAD_DIM
# - EMBED_DIM
# - MLP_DIM
from __gin__ import dynamic_registration

from flax import linen

from flaxformer.architectures.perceiver_ar import decoder_layer
from flaxformer.architectures.perceiver_ar import dense_attention
from flaxformer.architectures.perceiver_ar import perceiver_ar_architecture
from flaxformer.components import dense
from flaxformer.components import embedding
from flaxformer.components import layer_norm

# Use most architecture defaults from t5_1_1_flaxformer.
include 'flaxformer/t5x/configs/t5/architectures/t5_1_1_flaxformer.gin'

# Must be overridden.
NUM_LATENTS = %gin.REQUIRED

NUM_ENCODER_LAYERS = 0

# Architecture
ARCHITECTURE = @perceiver_ar_architecture.DecoderOnly()
perceiver_ar_architecture.DecoderOnly:
  num_latents = %NUM_LATENTS
  decoder_factory = @perceiver_ar_architecture.Decoder
  dtype = %ACTIVATION_DTYPE

# Decoder
perceiver_ar_architecture.Decoder:
  num_latents = %NUM_LATENTS
  num_layers = %NUM_DECODER_LAYERS
  layer_factory = @decoder_layer.DecoderLayer
  dropout_factory = %DROPOUT_FACTORY
  layer_norm_factory = @layer_norm.T5LayerNorm
  output_logits_factory = @output_logits/dense.DenseGeneral
  position_embedder_factory = None
  shared_relative_position_bias_factory = None
  dtype = %ACTIVATION_DTYPE
  token_embedder_factory = @embedding.Embed

# Decoder Layer
decoder_layer.DecoderLayer:
  self_attention = @dense_attention.MultiHeadDotProductAttention()
  encoder_decoder_attention = @dense_attention.MultiHeadDotProductAttention()
  mlp = @dense.MlpBlock()
  dropout_factory = %DROPOUT_FACTORY
  layer_norm_factory = @layer_norm.T5LayerNorm
  activation_partitioning_dims = %ACTIVATION_PARTITIONING_DIMS
  num_latents = %NUM_LATENTS

# Attention
dense_attention.MultiHeadDotProductAttention:
  num_heads = %NUM_HEADS
  dtype = %ACTIVATION_DTYPE
  head_dim = %HEAD_DIM
  kernel_init =  @attention_kernel_init/linen.initializers.variance_scaling()
  bias_init = %BIAS_INIT
  use_bias = False
  broadcast_dropout = True
  dropout_rate = %DROPOUT_RATE
attention_kernel_init/linen.initializers.variance_scaling:
  scale = %SCALE
  mode = 'fan_in'
  distribution = 'normal'

# Rotary Positional Embedding (RoPE)
dense_attention.MultiHeadDotProductAttention:
  use_rotary_embedding = True