from tokenizer_util import sentence_generator

train:
    model_name = 'morpheme_aware_unigram_32K'
    vocab_size = 32000
    n_unused_symbols = 1000
    n_extra_symbols = 100
    sentence_iterator = @sentence_generator.generate_retokenized_sentence_from_tfds()

generate_retokenized_sentence_from_tfds:
    dataset = 'kcbert_cleaned:1.0.0'
    split = 'train[:10%]'
    text_column = 'text'
    data_dir = 'gs://kc-moe/dataset/tfds'
