from __gin__ import dynamic_registration
import __main__ as train_script
from t5x import partitioning
import seqio
import tasks

include 't5x/examples/t5/t5_1_1/base.gin'
include 't5x/configs/runs/pretrain.gin'

MIXTURE_OR_TASK_NAME = 'kcbert_cleaned'
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 114}
TRAIN_STEPS = 131072 # 2^17 steps = 2^34(17B) tokens
DROPOUT_RATE = 0.0
BATCH_SIZE = 256
USE_CACHED_TASKS = False
MODEL_DIR = 'gs://kc-moe-eu/t5x/kct5-base'

seqio.SentencePieceVocabulary.sentencepiece_model_file = 'gs://kc-moe-eu/vocab/morpheme_aware_unigram_32k.100extra.model'

train_script.train:
  eval_period = 500
  random_seed = 42
  use_hardware_rng = True

seqio.Evaluator:
  logger_cls = [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger]

partitioning.PjitPartitioner.num_partitions = 2
