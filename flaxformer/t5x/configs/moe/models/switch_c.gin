# Switch Transformer C model.
#
# Based on the original Switch Transformer (https://arxiv.org/abs/2101.03961).
#
# Note that unlike the original Mesh Tensorflow Switch Transformer, this T5X
# version does not use any jitter noise in the router.
#
# Provides MODEL and NUM_EXPERTS.

from flaxformer.architectures.moe import moe_layers
from flaxformer.components import dense

include 'flaxformer/t5x/configs/moe/models/switch_base.gin'

# Architecture overrides
NUM_ENCODER_LAYERS = 15
NUM_DECODER_LAYERS = 15
NUM_HEADS = 30
HEAD_DIM = 64
EMBED_DIM = 2080
MLP_DIM = 6144   # All MLPs are MoE MLPs in C model; Smaller than XXL
GROUP_SIZE = 2048  # Smaller than XXL

# MoE overrides
NUM_EXPERTS = 2048
# All MLP sublayers are sparse MoE MLP sublayers.
NUM_ENCODER_SPARSE_LAYERS = %NUM_ENCODER_LAYERS
NUM_DECODER_SPARSE_LAYERS = %NUM_DECODER_LAYERS
TRAIN_EXPERT_CAPACITY_FACTOR = 1.   # Smaller than XXL
EVAL_EXPERT_CAPACITY_FACTOR = 2.   # Smaller than XXL
moe_layers.MoeLayer.min_expert_capacity = 2

# Switch Transformer C uses a separate output logits layer.
moe_architecture.SparseDecoder.output_logits_factory = @output_logits/dense.DenseGeneral

# As in the Base config, Switch Transformer C uses relu activations.
