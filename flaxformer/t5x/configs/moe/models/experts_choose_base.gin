# "Experts choose tokens" MoE Base model.
#
# Provides MODEL and NUM_EXPERTS.
#
# We use experts choose routing in the encoder and top-2 tokens choose routing
# in the decoder (recall that experts choose routing can break autoregressive
# behavior).

from __gin__ import dynamic_registration

from flaxformer.architectures.moe import moe_architecture
from flaxformer.architectures.moe import moe_layers
from flaxformer.architectures.moe import routing
import seqio
from t5x import adafactor
from t5x.contrib.moe import adafactor_utils
from t5x.contrib.moe import models

ARCHITECTURE = %gin.REQUIRED

include 'flaxformer/t5x/configs/moe/architectures/moe.gin'

# Architecture overrides
NUM_ENCODER_LAYERS = 12
NUM_DECODER_LAYERS = 12
NUM_HEADS = 12
HEAD_DIM = 64
EMBED_DIM = 768
MLP_DIM = 2048

# MoE overrides
NUM_EXPERTS = 64
# Replace every other dense MLP sublayer with MoE sublayer.
NUM_ENCODER_SPARSE_LAYERS = 6
NUM_DECODER_SPARSE_LAYERS = 6
TRAIN_EXPERT_CAPACITY_FACTOR = 1.
EVAL_EXPERT_CAPACITY_FACTOR = 1.
EXPERT_MLP_DIM = %MLP_DIM
# Router computations are performed on a per-group basis. Smaller group sizes
# are faster, but may be slightly less stable.
GROUP_SIZE = 4096
AUX_LOSS_FACTOR = 0.01
ROUTER_Z_LOSS_FACTOR = 0.0

# Experts choose routing can break autoregressive behavior, so we only use it in
# the encoder.
sparse_encoder/moe_layers.MoeLayer.router = @sparse_encoder/routing.ExpertsChooseMaskedRouter()
sparse_encoder/routing.ExpertsChooseMaskedRouter:
  router_weights = @routing.RouterWeights()
  jitter_noise = %JITTER_NOISE  # provided by moe.gin
  dtype = %MOE_TRUNCATED_DTYPE  # provided by moe.gin
  ignore_padding_tokens = False
# We use top-2 tokens choose routing in the decoder.
NUM_SELECTED_EXPERTS = 2  # Top-2 routing for decoder

SCALE = 0.1  # Small weight initialization for stability

# Vocabulary (shared by encoder and decoder)
VOCABULARY = @seqio.SentencePieceVocabulary()
seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model"
NUM_EMBEDDINGS = 32128  # vocab size rounded to a multiple of 128 for TPU efficiency

# Optimizer
# `learning_rate` is set by `Trainer.learning_rate_fn`.
OPTIMIZER = @adafactor.Adafactor()
adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
  logical_factor_rules = @adafactor_utils.logical_factor_rules()

# Loss HParam defaults
Z_LOSS = 0.0001
LABEL_SMOOTHING = 0.0
# Normalize loss to match other MoE models and balance router z-loss.
LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'

# Model
MODEL = @models.MoeEncoderDecoderModel()
models.MoeEncoderDecoderModel:
  module = %ARCHITECTURE  # provided by moe.gin
  input_vocabulary = %VOCABULARY
  output_vocabulary = %VOCABULARY
  optimizer_def = %OPTIMIZER
  z_loss = %Z_LOSS
  label_smoothing = %LABEL_SMOOTHING
  loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
  aux_loss_factor = %AUX_LOSS_FACTOR
  router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
